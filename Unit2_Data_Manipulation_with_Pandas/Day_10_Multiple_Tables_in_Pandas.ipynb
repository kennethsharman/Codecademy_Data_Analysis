{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: Multiple DataFrames\n",
    "In order to efficiently store data, we often spread related information across multiple tables.\n",
    "\n",
    "In script.py, we've loaded in three DataFrames: orders, products, and customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "orders = pd.read_csv('orders1.csv')\n",
    "products = pd.read_csv('products.csv')\n",
    "customers = pd.read_csv('customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by inspecting orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  quantity   timestamp\n",
      "0         1            2           3         1  2017-01-01\n",
      "1         2            2           2         3  2017-01-01\n",
      "2         3            3           1         1  2017-01-01\n",
      "3         4            3           2         2  2017-02-01\n",
      "4         5            3           3         3  2017-02-01\n",
      "5         6            1           4         2  2017-03-01\n",
      "6         7            1           1         1  2017-02-02\n",
      "7         8            1           4         1  2017-02-02\n"
     ]
    }
   ],
   "source": [
    "print(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id         description  price\n",
      "0           1      thing-a-ma-jig      5\n",
      "1           2  whatcha-ma-call-it     10\n",
      "2           3          doo-hickey      7\n",
      "3           4               gizmo      3\n"
     ]
    }
   ],
   "source": [
    "print(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now inspect customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id customer_name        address  phone_number\n",
      "0            1    John Smith   123 Main St.  212-123-4567\n",
      "1            2      Jane Doe  456 Park Ave.  949-867-5309\n",
      "2            3     Joe Schmo   798 Broadway  112-358-1321\n"
     ]
    }
   ],
   "source": [
    "print(customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the orders and products tables.\n",
    "\n",
    "What is the description of the product that was ordered in Order 3?\n",
    "\n",
    "Give your answer as a string assigned to the variable order_3_description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_3_description = 'thing-a-ma-jig'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the orders and customers tables.\n",
    "\n",
    "What is the phone_number of the customer in Order 5?\n",
    "\n",
    "Give your answer as a string assigned to the variable order_5_phone_number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_5_phone_number = '112-358-1321'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Merge II\n",
    "It is easy to do this kind of matching for one row, but hard to do it for multiple rows.\n",
    "\n",
    "Luckily, Pandas can efficiently do this for the entire table. We use the .merge method.\n",
    "\n",
    "The .merge method looks for columns that are common between two DataFrames and then looks for rows where those column's values are the same. It then combines the matching rows into a single row in a new table.\n",
    "\n",
    "We can call the pd.merge method with two tables like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.merge(orders, customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are an analyst Cool T-Shirts Inc. You are going to help them analyze some of their sales data.\n",
    "\n",
    "There are two DataFrames defined in the file script.py:\n",
    "\n",
    "    sales contains the monthly revenue for Cool T-Shirts Inc. It has two columns: month and revenue.\n",
    "    targets contains the goals for monthly revenue for each month. It has two columns: month and target.\n",
    "Create a new DataFrame sales_vs_targets which contains the merge of sales and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue\n",
      "0   January      300\n",
      "1  February      290\n",
      "2     March      310\n",
      "3     April      325\n",
      "4       May      475\n",
      "5      June      495\n",
      "      month  target\n",
      "0   January     310\n",
      "1  February     270\n",
      "2     March     300\n",
      "3     April     350\n",
      "4       May     475\n",
      "5      June     500\n",
      "      month  revenue  target\n",
      "0   January      300     310\n",
      "1  February      290     270\n",
      "2     March      310     300\n",
      "3     April      325     350\n",
      "4       May      475     475\n",
      "5      June      495     500\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv('sales.csv')\n",
    "print(sales)\n",
    "targets = pd.read_csv('targets.csv')\n",
    "print(targets)\n",
    "\n",
    "sales_vs_targets = pd.merge(sales, targets)\n",
    "print(sales_vs_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool T-Shirts Inc. wants to know the months when they crushed their targets.\n",
    "\n",
    "Select the rows from sales_vs_targets where revenue is greater than target. Save these rows to the variable crushing_it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue  target\n",
      "1  February      290     270\n",
      "2     March      310     300\n"
     ]
    }
   ],
   "source": [
    "crushing_it = sales_vs_targets[sales_vs_targets.revenue > sales_vs_targets.target]\n",
    "print(crushing_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Merge III\n",
    "In addition to using pd.merge, each DataFrame has its own merge method. For instance, if you wanted to merge orders with customers, you could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = orders.merge(customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces the same DataFrame as if we had called pd.merge(orders, customers).\n",
    "\n",
    "We generally use this when we are joining more than two DataFrames together because we can \"chain\" the commands. The following command would merge orders to customers, and then the resulting DataFrame to products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # big_df = orders.merge(customers).merge(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some more data from Cool T-Shirts Inc. The number of men's and women's t-shirts sold per month is in a file called men_women_sales.csv. Load this data into a DataFrame called men_women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  men  women\n",
      "0   January   30     35\n",
      "1  February   29     35\n",
      "2     March   31     29\n",
      "3     April   32     28\n",
      "4       May   47     50\n",
      "5      June   49     45\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv('sales.csv')\n",
    "targets = pd.read_csv('targets.csv')\n",
    "men_women = pd.read_csv('men_women_sales.csv')\n",
    "print(men_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all three DataFrames (sales, targets, and men_women) into one big DataFrame called all_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue  target  men  women\n",
      "0   January      300     310   30     35\n",
      "1  February      290     270   29     35\n",
      "2     March      310     300   31     29\n",
      "3     April      325     350   32     28\n",
      "4       May      475     475   47     50\n",
      "5      June      495     500   49     45\n"
     ]
    }
   ],
   "source": [
    "all_data = sales.merge(targets).merge(men_women)\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool T-Shirts Inc. thinks that they have more revenue in months where they sell more women's t-shirts.\n",
    "\n",
    "Select the rows of all_data where:\n",
    "\n",
    "    revenue is greater than target\n",
    "AND\n",
    "\n",
    "    women is greater than men\n",
    "Save your answer to the variable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      month  revenue  target  men  women\n",
      "1  February      290     270   29     35\n"
     ]
    }
   ],
   "source": [
    "results = all_data[(all_data.revenue > all_data.target)]\n",
    "results = results[results.women > results.men]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on Specific Columns\n",
    "In the previous example, the merge function \"knew\" how to combine tables based on the columns that were the same between two tables. For instance, products and orders both had a column called product_id. This won't always be true when we want to perform a merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npd.merge(\\n    orders,\\n    customers.rename(columns={'id': 'customer_id'}))\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pd.merge(\n",
    "    orders,\n",
    "    customers.rename(columns={'id': 'customer_id'}))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge orders and products using rename. Save your results to the variable orders_products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  quantity   timestamp  \\\n",
      "0         1            2           3         1  2017-01-01   \n",
      "1         5            3           3         3  2017-02-01   \n",
      "2         2            2           2         3  2017-01-01   \n",
      "3         4            3           2         2  2017-02-01   \n",
      "4         3            3           1         1  2017-01-01   \n",
      "5         7            1           1         1  2017-02-02   \n",
      "6         6            1           4         2  2017-03-01   \n",
      "7         8            1           4         1  2017-02-02   \n",
      "\n",
      "          description  price  \n",
      "0          doo-hickey      7  \n",
      "1          doo-hickey      7  \n",
      "2  whatcha-ma-call-it     10  \n",
      "3  whatcha-ma-call-it     10  \n",
      "4      thing-a-ma-jig      5  \n",
      "5      thing-a-ma-jig      5  \n",
      "6               gizmo      3  \n",
      "7               gizmo      3  \n"
     ]
    }
   ],
   "source": [
    "orders_products = pd.merge(orders,\\\n",
    "                          products.rename(columns={'id': 'product_id'}))\n",
    "print(orders_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on Specific Columns II\n",
    "In the previous exercise, we learned how to use rename to merge two DataFrames whose columns don't match.\n",
    "\n",
    "If we don't want to do that, we have another option. We could use the keywords left_on and right_on to specify which columns we want to perform the merge on. In the example below, the \"left\" table is the one that comes first (orders), and the \"right\" table is the one that comes second (customers). This syntax says that we should match the customer_id from orders to the id in customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npd.merge(\\n    orders,\\n    customers,\\n    left_on='customer_id',\\n    right_on='id')\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pd.merge(\n",
    "    orders,\n",
    "    customers,\n",
    "    left_on='customer_id',\n",
    "    right_on='id')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use this syntax, we'll end up with two columns called id, one from the first table and one from the second. Pandas won't let you have two columns with the same name, so it will change them to id_x and id_y.\n",
    "\n",
    "We could use the following code to make the suffixes reflect the table names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npd.merge(\\n    orders,\\n    customers,\\n    left_on='customer_id',\\n    right_on='id',\\n    suffixes=['_order', '_customer']\\n)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pd.merge(\n",
    "    orders,\n",
    "    customers,\n",
    "    left_on='customer_id',\n",
    "    right_on='id',\n",
    "    suffixes=['_order', '_customer']\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge orders and products using left_on and right_on. Use the suffixes _orders and _products. Save your results to the variable orders_products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\norders_products = pd.merge(\\n                          orders,\\n                          products,\\n                          left_on='product_id',\\n                          right_on='id',\\n                          suffixes=['_orders', '_products'])\\nprint(orders_products)\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "orders_products = pd.merge(\n",
    "                          orders,\n",
    "                          products,\n",
    "                          left_on='product_id',\n",
    "                          right_on='id',\n",
    "                          suffixes=['_orders', '_products'])\n",
    "print(orders_products)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mismatched Merges\n",
    "In our previous examples, there were always matching values when we were performing our merges. What happens when that isn't true?\n",
    "\n",
    "Let's imagine that our products table is out of date and is missing the newest product: Product 5. What happens when someone orders it?\n",
    "\n",
    "We've just released a new product with product_id equal to 5. People are ordering this product, but we haven't updated the products table.\n",
    "\n",
    "In script.py, you'll find two DataFrames: products and orders. Inspect these DataFrames using print.\n",
    "\n",
    "Notice that the third order in orders is for the mysterious new product, but that there is no product_id 5 in products.\n",
    "\n",
    "Merge orders and products and save it to the variable merged_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  quantity   timestamp\n",
      "0         1            2           3         1  2017-01-01\n",
      "1         2            2           2         3  2017-01-01\n",
      "2         3            3           1         1  2017-01-01\n",
      "3         4            3           2         2  2017-02-01\n",
      "4         5            3           3         3  2017-02-01\n",
      "5         6            1           4         2  2017-03-01\n",
      "6         7            1           1         1  2017-02-02\n",
      "7         8            1           4         1  2017-02-02\n",
      "   product_id         description  price\n",
      "0           1      thing-a-ma-jig      5\n",
      "1           2  whatcha-ma-call-it     10\n",
      "2           3          doo-hickey      7\n",
      "3           4               gizmo      3\n"
     ]
    }
   ],
   "source": [
    "print(orders)\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  customer_id  product_id  quantity   timestamp  \\\n",
      "0         1            2           3         1  2017-01-01   \n",
      "1         5            3           3         3  2017-02-01   \n",
      "2         2            2           2         3  2017-01-01   \n",
      "3         4            3           2         2  2017-02-01   \n",
      "4         3            3           1         1  2017-01-01   \n",
      "5         7            1           1         1  2017-02-02   \n",
      "6         6            1           4         2  2017-03-01   \n",
      "7         8            1           4         1  2017-02-02   \n",
      "\n",
      "          description  price  \n",
      "0          doo-hickey      7  \n",
      "1          doo-hickey      7  \n",
      "2  whatcha-ma-call-it     10  \n",
      "3  whatcha-ma-call-it     10  \n",
      "4      thing-a-ma-jig      5  \n",
      "5      thing-a-ma-jig      5  \n",
      "6               gizmo      3  \n",
      "7               gizmo      3  \n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(orders, products)\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Merge\n",
    "In the previous exercise, we saw that when we merge two DataFrames whose rows don't match perfectly, we lose the unmatched rows.\n",
    "\n",
    "This type of merge (where we only include matching rows) is called an inner merge. There are other types of merges that we can use when we want to keep information from the unmatched rows.\n",
    "\n",
    "There are two hardware stores in town: Store A and Store B. Store A's inventory is in DataFrame store_a and Store B's inventory is in DataFrame store_b. They have decided to merge into one big Super Store!\n",
    "\n",
    "Combine the inventories of Store A and Store B using an outer merge. Save the results to the variable store_a_b_outer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          item  store_a_inventory\n",
      "0       hammer                 12\n",
      "1  screwdriver                 15\n",
      "2        nails                200\n",
      "3       screws                350\n",
      "4          saw                  6\n",
      "5    duct tape                150\n",
      "6       wrench                 12\n",
      "7     pvc pipe                 54\n",
      "            item  store_b_inventory\n",
      "0         hammer                  6\n",
      "1          nails                250\n",
      "2            saw                  6\n",
      "3      duct tape                150\n",
      "4       pvc pipe                 54\n",
      "5           rake                 10\n",
      "6         shovel                 15\n",
      "7  wooden dowels                192\n",
      "             item  store_a_inventory  store_b_inventory\n",
      "0          hammer               12.0                6.0\n",
      "1     screwdriver               15.0                NaN\n",
      "2           nails              200.0              250.0\n",
      "3          screws              350.0                NaN\n",
      "4             saw                6.0                6.0\n",
      "5       duct tape              150.0              150.0\n",
      "6          wrench               12.0                NaN\n",
      "7        pvc pipe               54.0               54.0\n",
      "8            rake                NaN               10.0\n",
      "9          shovel                NaN               15.0\n",
      "10  wooden dowels                NaN              192.0\n"
     ]
    }
   ],
   "source": [
    "store_a = pd.read_csv('store_a.csv')\n",
    "print(store_a)\n",
    "store_b = pd.read_csv('store_b.csv')\n",
    "print(store_b)\n",
    "\n",
    "store_a_b_outer = pd.merge(store_a, store_b, how='outer')\n",
    "print(store_a_b_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Merge\n",
    "Let's return to the merge of Company A and Company B.\n",
    "\n",
    "Suppose we want to identify which customers are missing phone information. We would want a list of all customers who have email, but don't have phone.\n",
    "\n",
    "We could get this by performing a Left Merge. A Left Merge includes all rows from the first (left) table, but only rows from the second (right) table that match the first table.\n",
    "\n",
    "For this command, the order of the arguments matters. If the first DataFrame is company_a and we do a left join, we'll only end up with rows that appear in company_a.\n",
    "\n",
    "By listing company_a first, we get all customers from Company A, and only customers from Company B who are also customers of Company B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(company_a, company_b, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the two hardware stores, Store A and Store B. They're not quite sure if they want to merge into a big Super Store just yet.\n",
    "\n",
    "Store A wants to find out what products they carry that Store B does not carry. Using a left merge, combine store_a to store_b and save the results to store_a_b_left.\n",
    "\n",
    "The items with null in store_b_inventory are carried by Store A, but not Store B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_a_b_left = pd.merge(store_a, store_b, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Store B wants to find out what products they carry that Store A does not carry. Use a left join, to combine the two DataFrames but in the reverse order (i.e., store_b followed by store_a) and save the results to the variable store_b_a_left.\n",
    "\n",
    "Which items are not carried by Store A, but are carried by Store B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          item  store_a_inventory  store_b_inventory\n",
      "0       hammer                 12                6.0\n",
      "1  screwdriver                 15                NaN\n",
      "2        nails                200              250.0\n",
      "3       screws                350                NaN\n",
      "4          saw                  6                6.0\n",
      "5    duct tape                150              150.0\n",
      "6       wrench                 12                NaN\n",
      "7     pvc pipe                 54               54.0\n",
      "            item  store_b_inventory  store_a_inventory\n",
      "0         hammer                  6               12.0\n",
      "1          nails                250              200.0\n",
      "2            saw                  6                6.0\n",
      "3      duct tape                150              150.0\n",
      "4       pvc pipe                 54               54.0\n",
      "5           rake                 10                NaN\n",
      "6         shovel                 15                NaN\n",
      "7  wooden dowels                192                NaN\n"
     ]
    }
   ],
   "source": [
    "store_b_a_left = pd.merge(store_b, store_a, how='left')\n",
    "\n",
    "print(store_a_b_left)\n",
    "print(store_b_a_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate DataFrames\n",
    "Sometimes, a dataset is broken into multiple tables. For instance, data is often split into multiple CSV files so that each download is smaller.\n",
    "\n",
    "When we need to reconstruct a single DataFrame from multiple smaller DataFrames, we can use the method pd.concat([df1, df2, df2, ...]). This method only works if all of the columns are the same in all of the DataFrames.\n",
    "\n",
    "An ice cream parlor and a bakery have decided to merge.\n",
    "\n",
    "The bakery's menu is stored in the DataFrame bakery, and the ice cream parlor's menu is stored in DataFrame ice_cream.\n",
    "\n",
    "Create their new menu by concatenating the two DataFrames into a DataFrame called menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  item  price\n",
      "0               cookie   2.50\n",
      "1              brownie   3.50\n",
      "2        slice of cake   4.75\n",
      "3  slice of cheesecake   4.75\n",
      "4         slice of pie   5.00\n",
      "                              item  price\n",
      "0     scoop of chocolate ice cream   3.00\n",
      "1       scoop of vanilla ice cream   2.95\n",
      "2    scoop of strawberry ice cream   3.05\n",
      "3  scoop of cookie dough ice cream   3.25\n",
      "                              item  price\n",
      "0                           cookie   2.50\n",
      "1                          brownie   3.50\n",
      "2                    slice of cake   4.75\n",
      "3              slice of cheesecake   4.75\n",
      "4                     slice of pie   5.00\n",
      "0     scoop of chocolate ice cream   3.00\n",
      "1       scoop of vanilla ice cream   2.95\n",
      "2    scoop of strawberry ice cream   3.05\n",
      "3  scoop of cookie dough ice cream   3.25\n"
     ]
    }
   ],
   "source": [
    "bakery = pd.read_csv('bakery.csv')\n",
    "print(bakery)\n",
    "ice_cream = pd.read_csv('ice_cream.csv')\n",
    "print(ice_cream)\n",
    "\n",
    "menu = pd.concat([bakery, ice_cream])\n",
    "print(menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "This lesson introduced some methods for combining multiple DataFrames:\n",
    "\n",
    "    Creating a DataFrame made by matching the common columns of two DataFrames is called a merge\n",
    "    We can specify which columns should be matches by using the keyword arguments left_on and right_on\n",
    "    We can combine DataFrames whose rows don't all match using left, right, and outer merges and the how keyword argument\n",
    "    We can stack or concatenate DataFrames with the same columns using pd.concat\n",
    "\n",
    "Cool T-Shirts Inc. just created a website for ordering their products. They want you to analyze two datasets for them:\n",
    "\n",
    "visits contains information on all visits to their landing page\n",
    "checkouts contains all users who began to checkout on their website\n",
    "Use print to inspect each DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 user_id          visit_time\n",
      "0   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:16:00\n",
      "1   7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:37:00\n",
      "2   0b061e73-f709-42fa-8d1a-5f68176ff154 2017-04-12 19:32:00\n",
      "3   9133d6f0-e68b-4c8d-bafd-ff2825e8dafe 2017-08-18 04:32:00\n",
      "4   08d13edb-071c-4cfb-9ee4-8f377d0e932a 2017-07-08 06:24:00\n",
      "5   c7192ab9-e033-4b69-971d-4bd92631342e 2017-10-05 09:16:00\n",
      "6   c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:19:00\n",
      "7   f028e9dd-77d0-4002-83f6-372a4837fda6 2017-10-27 08:46:00\n",
      "8   e43cf28f-7d08-4019-bd66-ddf7dfd2e034 2017-11-12 01:47:00\n",
      "9   746631d2-35d5-441e-a21b-e5f39442f981 2017-06-19 23:34:00\n",
      "10  a0fc94a2-4a80-4a33-994b-75783066ac62 2017-05-11 13:07:00\n",
      "11  e2c24ee0-7fdf-4400-abde-b36378fe5ce6 2017-07-04 15:33:00\n",
      "12  78751233-c0de-44fb-bc2f-822bd9dd9be7 2017-01-23 05:38:00\n",
      "13  fbcec4bc-f191-4c0c-870b-d22728ad1b18 2017-01-24 17:41:00\n",
      "14  e6c7ecb9-4710-4cbd-ad02-c43971ebbe7f 2017-09-27 16:10:00\n",
      "15  0c682ddd-144a-4743-9c82-dbd942fdea52 2017-08-15 23:38:00\n",
      "16  fe07fc99-3943-4000-99a5-422957a42ea1 2017-10-07 15:08:00\n",
      "17  bb6d2daf-bcf2-4970-b45d-33402cc1c45d 2017-07-04 21:22:00\n",
      "18  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:28:00\n",
      "19  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:01:00\n",
      "20  9e744240-18e8-4da0-9e05-89b116245c15 2017-11-02 20:01:00\n",
      "21  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:15:00\n",
      "22  f8123e12-d349-4efe-ae65-9494630bee6c 2017-02-22 13:16:00\n",
      "23  6aee3eda-65f2-437f-88b2-cf7e2b94fcdc 2017-10-17 10:17:00\n",
      "24  ee9f8ae4-5450-4c92-ae54-edd44cd0482e 2017-05-12 14:29:00\n",
      "25  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:26:00\n",
      "26  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 11:39:00\n",
      "27  c54a72a7-42db-4d19-b897-944f37cf386d 2017-01-02 17:20:00\n",
      "28  d7a50dcf-e6d7-44af-87a7-9cc68f303ece 2017-11-05 08:15:00\n",
      "29  65599d0d-76c2-4ad4-a717-70f12a187f1a 2017-02-15 07:21:00\n",
      "..                                   ...                 ...\n",
      "70  280c9dcb-2c9f-4f33-ada1-92196c0b1d37 2017-04-10 14:55:00\n",
      "71  f74519df-e961-4841-acdb-2d47da194aba 2017-11-17 21:05:00\n",
      "72  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 12:50:00\n",
      "73  0fbf570b-bc50-4f9c-ad63-9070e7d888b6 2017-08-08 23:50:00\n",
      "74  6a504e55-b59c-416d-beda-4194c89066e6 2017-02-25 15:54:00\n",
      "75  fe90a9f4-960a-4a0d-9160-e562adb79365 2017-11-09 09:04:00\n",
      "76  52b650a4-f315-4947-804f-19df5f971d85 2017-07-05 21:15:00\n",
      "77  9812465d-fded-43c4-8685-3e96446f6cc7 2017-07-19 17:02:00\n",
      "78  66b4a07f-224b-4f83-91b2-b2d27c9dbe73 2017-06-23 14:17:00\n",
      "79  f41d2868-515d-49a2-b48c-e4f33e5d9b69 2017-09-07 16:40:00\n",
      "80  e0e3191d-4adb-4b43-8815-0ab43815c4d7 2017-11-27 16:30:00\n",
      "81  43db76fc-d522-450d-a371-ef2a683d5bfd 2017-03-26 21:11:00\n",
      "82  1a35b7eb-f603-407d-91be-a2c3304066fd 2017-08-15 21:09:00\n",
      "83  27041dde-19da-4571-9a69-46ca3619b44c 2017-09-08 12:12:00\n",
      "84  b9644df3-e2f6-4226-8df1-ac1945952494 2017-03-16 10:13:00\n",
      "85  8b4dbf70-db73-4bc6-88fe-fc8aaeae9277 2017-11-22 15:02:00\n",
      "86  b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:13:00\n",
      "87  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:42:00\n",
      "88  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:10:00\n",
      "89  23a8d1be-3f5c-4b59-aed7-c7f19c51612b 2017-08-11 13:49:00\n",
      "90  c09b76e2-3a95-426c-ae76-c1e0f6a15aee 2017-06-07 12:24:00\n",
      "91  2da32318-0fd3-4432-85da-cb55e3bdd2ec 2017-07-16 10:11:00\n",
      "92  a9def5d7-dba9-4175-a2da-98c2d2854984 2017-08-11 07:29:00\n",
      "93  41efeead-9983-45f3-bec1-44887a03f6ab 2017-09-26 14:22:00\n",
      "94  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:23:00\n",
      "95  442efd1c-8d7b-4d6a-83be-8f2a9e08b34f 2017-02-19 11:20:00\n",
      "96  5679519b-a901-4970-8656-dbf60ffb618d 2017-07-20 04:23:00\n",
      "97  26deb2d5-1d7e-4774-bf6e-1df2ee9ee59d 2017-09-06 07:29:00\n",
      "98  fff8f87a-e4a2-4f2c-b3d4-93a4ece95c4f 2017-06-06 23:42:00\n",
      "99  42617776-2850-431d-9262-a9c11e5ad17f 2017-02-10 10:49:00\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "                                 user_id       checkout_time\n",
      "0   fe90a9f4-960a-4a0d-9160-e562adb79365 2017-11-09 09:25:00\n",
      "1   1a35b7eb-f603-407d-91be-a2c3304066fd 2017-08-15 21:25:00\n",
      "2   e2c24ee0-7fdf-4400-abde-b36378fe5ce6 2017-07-04 15:39:00\n",
      "3   10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:07:00\n",
      "4   f028e9dd-77d0-4002-83f6-372a4837fda6 2017-10-27 08:57:00\n",
      "5   b7953447-00a8-42be-99d2-b511f4e9c12b 2017-04-24 10:20:00\n",
      "6   280c9dcb-2c9f-4f33-ada1-92196c0b1d37 2017-04-10 14:57:00\n",
      "7   319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:27:00\n",
      "8   08d13edb-071c-4cfb-9ee4-8f377d0e932a 2017-07-08 06:32:00\n",
      "9   cb602c66-1366-467a-8bee-52477310cf42 2017-07-03 06:32:00\n",
      "10  62173c69-bec8-4a5d-951c-b860df94adc2 2017-05-04 00:08:00\n",
      "11  34b2bf31-9ce0-4bb7-8f18-624ff4f4e904 2017-02-13 15:39:00\n",
      "12  2d747792-db15-4549-abb8-ef352884b3db 2017-01-16 18:30:00\n",
      "13  ccd0db8b-5e94-4103-a0b0-14ba5289c6f3 2017-08-19 06:12:00\n",
      "14  7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:49:00\n",
      "15  147d51cd-95d7-48f7-871d-112e7e7f47fb 2017-06-09 14:09:00\n",
      "16  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 19:49:00\n",
      "17  f8123e12-d349-4efe-ae65-9494630bee6c 2017-02-22 13:44:00\n",
      "18  fe07fc99-3943-4000-99a5-422957a42ea1 2017-10-07 15:25:00\n",
      "19  2f1e93f2-4d40-45e1-8371-7c65660f6bf9 2017-10-25 20:05:00\n",
      "20  ad7a3539-ad48-4b0e-bcc3-8c0dff722908 2017-06-13 08:40:00\n",
      "21  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:19:00\n",
      "22  5679519b-a901-4970-8656-dbf60ffb618d 2017-07-20 04:24:00\n",
      "23  fff8f87a-e4a2-4f2c-b3d4-93a4ece95c4f 2017-06-07 00:11:00\n",
      "24  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:48:00\n",
      "25  147d51cd-95d7-48f7-871d-112e7e7f47fb 2017-06-09 14:09:00\n",
      "26  10dbd3c5-d610-44e9-9994-110a7950b6b4 2017-08-09 21:04:00\n",
      "27  9e744240-18e8-4da0-9e05-89b116245c15 2017-11-02 20:04:00\n",
      "28  d2ae16fa-abc6-4755-af80-5010a7c6a103 2017-04-19 03:21:00\n",
      "29  851b52d1-31e9-468d-834f-5363fee108ac 2017-09-21 22:06:00\n",
      "..                                   ...                 ...\n",
      "50  3a46dc74-4505-4a1a-8dde-4268a7484321 2017-09-17 23:00:00\n",
      "51  23a8d1be-3f5c-4b59-aed7-c7f19c51612b 2017-08-11 14:11:00\n",
      "52  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:24:00\n",
      "53  cc90d623-e2d8-468b-aa8b-912d7b9b67c8 2017-02-17 18:22:00\n",
      "54  8d9ac96c-16be-418e-8df4-1a6202d0b36e 2017-10-07 10:52:00\n",
      "55  62173c69-bec8-4a5d-951c-b860df94adc2 2017-05-04 00:07:00\n",
      "56  cc90d623-e2d8-468b-aa8b-912d7b9b67c8 2017-02-17 18:27:00\n",
      "57  0d798d59-fb81-40ae-8ab1-d4beaffe8715 2017-01-18 17:38:00\n",
      "58  34b2bf31-9ce0-4bb7-8f18-624ff4f4e904 2017-02-13 15:25:00\n",
      "59  6d746f96-30eb-49df-8131-9ab61e8b9b20 2017-06-20 07:54:00\n",
      "60  287c13f5-9a9a-4c3a-9f0d-13853e8aad9b 2017-03-27 23:44:00\n",
      "61  f41d2868-515d-49a2-b48c-e4f33e5d9b69 2017-09-07 16:47:00\n",
      "62  67905d3e-5332-4927-a8ca-7ae9daabc7b8 2017-05-20 03:44:00\n",
      "63  31f1be0d-4502-4616-a7d6-a90163b8bb54 2017-11-11 03:57:00\n",
      "64  d722f212-04e9-482d-8f72-9d0df4cc8e6a 2017-02-21 20:03:00\n",
      "65  c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:39:00\n",
      "66  3e5aa90a-279f-4ef8-99a2-9262604e9acc 2017-04-14 04:25:00\n",
      "67  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 13:09:00\n",
      "68  f5f90dcf-15a0-432b-9886-9e4b5907a1cc 2017-07-10 12:03:00\n",
      "69  a0fc94a2-4a80-4a33-994b-75783066ac62 2017-05-11 13:31:00\n",
      "70  65599d0d-76c2-4ad4-a717-70f12a187f1a 2017-02-15 07:27:00\n",
      "71  b2b04aa6-3ef1-46d6-939b-215126b4b91b 2017-05-03 13:25:00\n",
      "72  c4dac0f2-2fa9-48a8-b056-c3b2a5a5c683 2017-07-09 14:42:00\n",
      "73  0b6b993f-3364-46ee-9baa-12b7fa546927 2017-03-06 23:59:00\n",
      "74  9812465d-fded-43c4-8685-3e96446f6cc7 2017-07-19 17:15:00\n",
      "75  7fe800cc-46e8-427c-a7af-f27198d305a1 2017-01-18 13:14:00\n",
      "76  43db76fc-d522-450d-a371-ef2a683d5bfd 2017-03-26 21:31:00\n",
      "77  851b52d1-31e9-468d-834f-5363fee108ac 2017-09-21 22:24:00\n",
      "78  7435ec9f-576d-4ebd-8791-361b128fca77 2017-05-16 08:55:00\n",
      "79  319350b4-9951-47ef-b3a7-6b252099905f 2017-02-21 07:40:00\n",
      "\n",
      "[80 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "visits = pd.read_csv('visits.csv',\n",
    "                        parse_dates=[1])\n",
    "checkouts = pd.read_csv('checkouts.csv',\n",
    "                        parse_dates=[1])\n",
    "print(visits)\n",
    "print(checkouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know the amount of time from a user's initial visit to the website to when they start to check out.\n",
    "\n",
    "Use merge to combine visits and checkouts and save it to the variable v_to_c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_to_c = pd.merge(visits, checkouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the time between visiting and checking out, define a column of v_to_c called time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_to_c['time'] = v_to_c.checkout_time - v_to_c.visit_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average time to checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.mean of 0    00:11:00\n",
      "1    00:24:00\n",
      "2    00:12:00\n",
      "3    00:18:00\n",
      "4    00:08:00\n",
      "5    00:07:00\n",
      "6    00:05:00\n",
      "7    00:20:00\n",
      "8    00:23:00\n",
      "9    00:11:00\n",
      "10   00:18:00\n",
      "11   00:13:00\n",
      "12   00:24:00\n",
      "13   00:06:00\n",
      "14   00:20:00\n",
      "15   00:17:00\n",
      "16   00:12:00\n",
      "17   00:22:00\n",
      "18   00:06:00\n",
      "19   00:03:00\n",
      "20   00:03:00\n",
      "21   00:04:00\n",
      "22   00:00:00\n",
      "23   00:10:00\n",
      "24   00:28:00\n",
      "25   00:22:00\n",
      "26   00:12:00\n",
      "27   00:29:00\n",
      "28   00:24:00\n",
      "29   00:06:00\n",
      "       ...   \n",
      "50   00:22:00\n",
      "51   00:26:00\n",
      "52   00:19:00\n",
      "53   00:16:00\n",
      "54   00:15:00\n",
      "55   00:08:00\n",
      "56   00:01:00\n",
      "57   00:14:00\n",
      "58   00:02:00\n",
      "59   00:03:00\n",
      "60   00:19:00\n",
      "61   00:24:00\n",
      "62   00:21:00\n",
      "63   00:27:00\n",
      "64   00:13:00\n",
      "65   00:07:00\n",
      "66   00:20:00\n",
      "67   00:16:00\n",
      "68   00:07:00\n",
      "69   00:13:00\n",
      "70   00:07:00\n",
      "71   00:23:00\n",
      "72   00:20:00\n",
      "73   00:00:00\n",
      "74   00:29:00\n",
      "75   00:22:00\n",
      "76   00:01:00\n",
      "77   00:29:00\n",
      "78   00:01:00\n",
      "79   00:29:00\n",
      "Name: time, Length: 80, dtype: timedelta64[ns]>\n"
     ]
    }
   ],
   "source": [
    "print(v_to_c.time.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
